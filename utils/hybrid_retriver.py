from typing import List
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from langchain_chroma import Chroma
from pydantic import Field
from utils.search import SearxngClient
from configs.settings import SEARCH


class HybridRetriever(BaseRetriever):
    """æ”¯æŒæœ¬åœ°+ç½‘ç»œæœç´¢çš„æ··åˆæ£€ç´¢å™¨"""
    vector_db: Chroma = Field(...)
    searxng_client: SearxngClient = Field(...)
    enable_web: bool = Field(default=True)
    top_k: int = Field(default=SEARCH["top_k"])

    # Chroma
    def _get_local_docs(self, query: str) -> List[Document]:
        # æ–°ç‰ˆ Chroma çš„æ­£ç¡®è°ƒç”¨æ–¹å¼
        print(f"ğŸ” çŸ¥è¯†åº“æ£€ç´¢")
        docs_with_scores = self.vector_db.similarity_search_with_score(
            query=query,
            k=5,
        )
        # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
        similarity_threshold = 0.3  # ç›¸ä¼¼åº¦åˆ†æ•°é˜ˆå€¼ï¼ˆåˆ†æ•°è¶Šä½è¡¨ç¤ºè¶Šç›¸ä¼¼ï¼‰

        # åŒé‡è¿‡æ»¤ï¼šåˆ†æ•°é˜ˆå€¼ + æœ€å¤§è¿”å›æ•°é‡
        filtered_docs = [doc for doc, score in docs_with_scores if score <= similarity_threshold][:3]  # æœ€ç»ˆè¿”å› top_k ä¸ª

        return filtered_docs

    # Elasticsearch
    def _get_es_local_docs(self, query: str) -> List[Document]:
        # Elasticsearch ç›¸ä¼¼åº¦æŸ¥è¯¢
        search_body = {
            "knn": {
                "field": "vector",
                "query_vector": self.vector_store.embedding.embed_query(query),
                "k": 5,
                "num_candidates": 50
            },
            "fields": ["content", "metadata"]
        }

        response = self.vector_store.client.search(
            index=self.vector_store.index_name,
            body=search_body
        )

        return [
            Document(
                page_content=hit["_source"]["content"],
                metadata=hit["_source"].get("metadata", {})
            ) for hit in response["hits"]["hits"]
        ]

    def _get_web_docs(self, query: str) -> List[Document]:
        """ç½‘ç»œæœç´¢æ£€ç´¢"""
        if not self.enable_web:
            return []

        try:
            print(f"ğŸ” ç½‘ç»œæœç´¢")
            results = self.searxng_client.search(query)
            return self.searxng_client.to_documents(results)[:self.top_k]
        except Exception as e:
            print(f"âš ï¸ ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
            return []

    def _merge_results(self, local_docs: List[Document], web_docs: List[Document]) -> List[Document]:
        # å¼ºåˆ¶å…ƒæ•°æ®è¡¥å…¨
        for doc in local_docs + web_docs:
            doc.metadata.setdefault("source", "local" if doc in local_docs else "web")
            doc.metadata.setdefault("url", "N/A")

        # ä¼˜å…ˆä¿ç•™ç½‘ç»œç»“æœï¼Œå¹¶æŒ‰ç›¸å…³æ€§æ’åº
        combined = sorted(
            web_docs + local_docs,
            key=lambda x: (
                -x.metadata.get("rank", 0) if x.metadata["source"] == "web" else 0,
                len(x.page_content)
            ),
            reverse=True
        )

        # åŸºäºæ ‡é¢˜å’Œé¦–æ®µå†…å®¹å»é‡
        seen = set()
        merged = []
        for doc in combined:
            identifier = f"{doc.metadata.get('title', '')[:30]}_{hash(doc.page_content[:200])}"
            if identifier not in seen:
                seen.add(identifier)
                merged.append(doc)
        return merged[:12]

    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:
        """æ ¸å¿ƒæ£€ç´¢é€»è¾‘"""
        # å¹¶è¡Œæ‰§è¡Œæœ¬åœ°å’Œç½‘ç»œæ£€ç´¢
        local_docs = self._get_local_docs(query)
        web_docs = self._get_web_docs(query)

        # åˆå¹¶ç»“æœ
        return self._merge_results(local_docs, web_docs)